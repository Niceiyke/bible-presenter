{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible Presenter — Stacked Embeddings Generator\n",
    "\n",
    "This notebook generates `all_versions_embeddings.npy` for the Bible Presenter app.\n",
    "\n",
    "**Before running:**\n",
    "1. In the top menu go to **Runtime → Change runtime type → T4 GPU**\n",
    "2. Then click **Runtime → Run all**\n",
    "3. At the end a download of `all_versions_embeddings.npy` will start automatically\n",
    "\n",
    "Versions embedded: `KJV, AMP, NIV, ESV, NKJV, NASB`  \n",
    "Expected output size: ~287 MB  \n",
    "Expected time with T4 GPU: ~5–10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 1: Check GPU ────────────────────────────────────────────────────────\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print('GPU detected:', result.stdout.strip())\n",
    "else:\n",
    "    print('WARNING: No GPU detected. Go to Runtime → Change runtime type → T4 GPU')\n",
    "    print('Continuing on CPU will take ~1-2 hours instead of ~5 minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 2: Install dependencies ─────────────────────────────────────────────\n",
    "!pip install -q sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 3: Download super_bible.db ──────────────────────────────────────────\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "DB_URL = 'https://raw.githubusercontent.com/alshival/super_bible/main/SUPER_BIBLE/super_bible.db'\n",
    "DB_PATH = 'super_bible.db'\n",
    "\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print('Downloading super_bible.db (~59 MB)...')\n",
    "    urllib.request.urlretrieve(DB_URL, DB_PATH)\n",
    "    print(f'Done. Size: {os.path.getsize(DB_PATH) / 1e6:.1f} MB')\n",
    "else:\n",
    "    print(f'Already exists. Size: {os.path.getsize(DB_PATH) / 1e6:.1f} MB')\n",
    "\n",
    "# Quick sanity check\n",
    "import sqlite3\n",
    "db = sqlite3.connect(DB_PATH)\n",
    "versions = [r[0] for r in db.execute(\"SELECT DISTINCT version FROM super_bible WHERE language='EN' ORDER BY version\").fetchall()]\n",
    "print('Available EN versions:', versions)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: Generate stacked embeddings ──────────────────────────────────────\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Versions to embed — must match EMBEDDED_VERSIONS in src-tauri/src/store/mod.rs\n",
    "VERSIONS = ['KJV', 'AMP', 'NIV', 'ESV', 'NKJV', 'NASB']\n",
    "OUT_NPY  = 'all_versions_embeddings.npy'\n",
    "OUT_IDX  = 'verse_index.json'\n",
    "\n",
    "print('Loading model all-MiniLM-L6-v2...')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print('Model ready.')\n",
    "\n",
    "db = sqlite3.connect(DB_PATH)\n",
    "\n",
    "available = {r[0] for r in db.execute(\"SELECT DISTINCT version FROM super_bible WHERE language='EN'\")}\n",
    "versions_to_use = [v for v in VERSIONS if v in available]\n",
    "missing = [v for v in VERSIONS if v not in available]\n",
    "if missing:\n",
    "    print(f'WARNING: versions not in DB, skipping: {missing}')\n",
    "print(f'Embedding versions: {versions_to_use}')\n",
    "\n",
    "all_embeddings = []\n",
    "verse_index    = []\n",
    "\n",
    "for version in versions_to_use:\n",
    "    print(f'\\n── {version} ──')\n",
    "    rows = db.execute(\n",
    "        'SELECT title, chapter, verse, text FROM super_bible '\n",
    "        'WHERE version = ? AND language = ? ORDER BY book, chapter, verse',\n",
    "        (version, 'EN')\n",
    "    ).fetchall()\n",
    "    print(f'  {len(rows)} verses')\n",
    "\n",
    "    texts = [r[3] for r in rows]\n",
    "    embs  = model.encode(\n",
    "        texts,\n",
    "        normalize_embeddings=True,\n",
    "        batch_size=512,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device='cuda' if __import__('torch').cuda.is_available() else 'cpu',\n",
    "    )\n",
    "    all_embeddings.append(embs.astype(np.float32))\n",
    "    for r in rows:\n",
    "        verse_index.append({'book': r[0], 'chapter': int(r[1]), 'verse': int(r[2]), 'version': version})\n",
    "\n",
    "db.close()\n",
    "\n",
    "stacked = np.vstack(all_embeddings)\n",
    "print(f'\\nStacked shape: {stacked.shape}  ({stacked.nbytes / 1e6:.1f} MB)')\n",
    "np.save(OUT_NPY, stacked)\n",
    "print(f'Saved → {OUT_NPY}')\n",
    "\n",
    "import json\n",
    "with open(OUT_IDX, 'w') as f:\n",
    "    json.dump(verse_index, f, separators=(',', ':'))\n",
    "print(f'Saved → {OUT_IDX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 5: Download the files ────────────────────────────────────────────────\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "npy_size = os.path.getsize('all_versions_embeddings.npy') / 1e6\n",
    "idx_size = os.path.getsize('verse_index.json') / 1e6\n",
    "print(f'all_versions_embeddings.npy  {npy_size:.1f} MB')\n",
    "print(f'verse_index.json             {idx_size:.1f} MB')\n",
    "print('\\nStarting download...')\n",
    "\n",
    "files.download('all_versions_embeddings.npy')\n",
    "files.download('verse_index.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After downloading\n",
    "\n",
    "Place both files in your project:\n",
    "\n",
    "```\n",
    "desktop-rs/src-tauri/bible_data/all_versions_embeddings.npy\n",
    "desktop-rs/src-tauri/bible_data/verse_index.json\n",
    "```\n",
    "\n",
    "Then upload `all_versions_embeddings.npy` to a **GitHub Release** (it's ~287 MB, too large for a regular commit):\n",
    "\n",
    "1. Go to your repo → **Releases → Draft a new release**\n",
    "2. Tag: `v1.0-models` (or similar)\n",
    "3. Attach `all_versions_embeddings.npy` as a release asset\n",
    "4. Publish\n",
    "\n",
    "The `verse_index.json` (~3 MB) can be committed normally.\n",
    "\n",
    "You're done!"
   ]
  }
 ]
}
